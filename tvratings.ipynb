{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5bd5131",
   "metadata": {},
   "source": [
    "'''\n",
    "This is the definition of the notebook\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83c3ca",
   "metadata": {},
   "source": [
    "## Importing Dataset and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d02cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Required libraries. Some of them may not be required.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "import pandas as pd #req.\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import xlsxwriter\n",
    "import re\n",
    "import string\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "import logging\n",
    "import string\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords, wordnet # req.\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from gensim.models import Word2Vec\n",
    "from zemberek import ( #req.\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import gensim\n",
    "import zeyrek\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "from gensim.matutils import cossim\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d836a",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8427d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('tvseries_full.csv', delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d920fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Lab.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data.Lab != 'Junk']\n",
    "\n",
    "df.Lab.replace(\"Neutral-Negative\", \n",
    "           \"Neutral\", \n",
    "           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9265f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Lab.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf0566",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81c5c6",
   "metadata": {},
   "source": [
    "##### Removing Mentions, Punctuations, HTMLs, Hyperlinks and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b883c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].replace('@[A-Za-z0-9]+', '', regex=True).replace('@[A-Za-z0-9]+', '', regex=True)\n",
    "df['Content']  = df['Content'] .replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "df['Content']  = df['Content'] .replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "df['Content']  = df['Content'] .replace('#[A-Za-z0-9ğüşöçıİĞÜŞÖÇ]+', \n",
    "                                  '', regex=True).replace('#[A-Za-z0-9ğüşöçıİĞÜŞÖÇ]+', '', regex=True)\n",
    "df['Content'] = df['Content'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "\n",
    "#repeated letters and spaces\n",
    "ss=df['Content'] .copy()\n",
    "new_st = []\n",
    "i = 0\n",
    "for k in ss:\n",
    "    new_ss = re.sub(r'([A-Za-z0-9ğüşöçıİĞÜŞÖÇ])\\1+', r'\\1',k)\n",
    "    new_st.append(new_ss)\n",
    "    i += 1\n",
    "\n",
    "df['Content'] = new_st\n",
    "\n",
    "df['Content']  = df['Content'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "extractor = TurkishSentenceExtractor()\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "ss2 = df['Content'] .copy()\n",
    "new_st2 = []\n",
    "for k in ss2:\n",
    "    words = k.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    new_st2.append(stripped)\n",
    "    \n",
    "df['tokenized'] = new_st2\n",
    "\n",
    "# lowercase\n",
    "\n",
    "df['lower'] = df['tokenized'].apply(\n",
    "    lambda x: [word.lower() for word in x])\n",
    "\n",
    "\n",
    "#stopwords\n",
    "\n",
    "stops = set(stopwords.words('turkish'))\n",
    "\n",
    "df['stopwords_removed'] = df['lower'].apply(\n",
    "    lambda x: [word for word in x if word not in stops])\n",
    "\n",
    "\n",
    "# Applying part of speech tags.\n",
    "\n",
    "df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "# Converting part of speeches to wordnet format.\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "df['wordnet_pos'] = df['pos_tags'].apply(\n",
    "    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "#  normalization\n",
    "\n",
    "def normalize_long_text(text):\n",
    "    normalized_sentences = [normalizer.normalize(word) for word in text]\n",
    "    normalized_text = \" \".join(normalized_sentences)\n",
    "    return normalized_text\n",
    "\n",
    "sentences = df['tokenized'].copy()\n",
    "new_sent = []\n",
    "start = time.time()\n",
    "\n",
    "for token in sentences:   \n",
    "    if token.count('') > 0:\n",
    "        token = list(filter(('').__ne__, token))\n",
    "    new_token = normalize_long_text(token)\n",
    "    new_sent.append(new_token)\n",
    "\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")\n",
    "\n",
    "splitted_words = []\n",
    "for sent in new_sent:\n",
    "    words = sent.split()\n",
    "    splitted_words.append(words)\n",
    "    \n",
    "for token in splitted_words:\n",
    "    j = 0\n",
    "    for word in token:\n",
    "        new_word = word.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", '')\n",
    "        token[j] = new_word\n",
    "        j += 1\n",
    "        \n",
    "        \n",
    "# Zeyrek for lemmatization\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "lem_sent = []\n",
    "for sent in splitted_words:\n",
    "    normalized_sent = []\n",
    "    for word in sent:\n",
    "        if word == '':\n",
    "            continue\n",
    "        else:\n",
    "            lem_word = analyzer.lemmatize(word)\n",
    "            normalized_sent.append(lem_word[0][1][0])\n",
    "    lem_sent.append(normalized_sent)\n",
    "    \n",
    "x = lem_sent.copy()\n",
    "for sent in x:\n",
    "    i = 0\n",
    "    for token in sent:\n",
    "        sent[i] = token.lower()\n",
    "        i += 1\n",
    "lem_sent = x\n",
    "\n",
    "\n",
    "lem_sent = list(filter(('').__ne__, lem_sent))\n",
    "df['lemmatized'] = lem_sent\n",
    "df['lemma_str'] = [' '.join(map(str, l)) for l in df['lemmatized']]\n",
    "\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829226ba",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.Lab)\n",
    "df['categorical_label'] = le.transform(df.Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for idx in df.index:\n",
    "    X.append(df['lemmatized'][idx])\n",
    "    Y.append(df['categorical_label'][idx])\n",
    "# Train-Test splitting\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "labels = ['Negative','Neutral', 'Positive']  # For further use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This utility function will be used to evaluate the other models also.\n",
    "def show_performance_data(Y_test, Y_pred, model_name):\n",
    "    print(classification_report(Y_test, Y_pred, target_names=labels))\n",
    "    tmp_result = classification_report(Y_test, Y_pred, target_names=labels, output_dict=True)\n",
    "    cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "    df_cm = pd.DataFrame(cm1, index = [i for i in labels], columns = [i for i in labels])\n",
    "    plt.figure(figsize = (7,5))\n",
    "    sns.heatmap(df_cm, annot=True,cmap='gist_earth_r', fmt='g')\n",
    "    plt.savefig('confusion_mrtx_'+model_name+'.png',bbox_inches = 'tight')\n",
    "    return tmp_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = 20\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "dictionary = Dictionary(X_train)\n",
    "train_corpus = [dictionary.doc2bow(doc) for doc in X_train]\n",
    "model = Lda(corpus=train_corpus, id2word=dictionary, num_topics=topic)\n",
    "test_corpus = [dictionary.doc2bow(doc) for doc in X_test]\n",
    "train_x_topics, test_x_topics = [], []\n",
    "# Convert text data into topic vectors\n",
    "for t in train_corpus:\n",
    "    train_x_topics.append(model[t])\n",
    "for t in test_corpus:\n",
    "    test_x_topics.append(model[t])\n",
    "# Prediction\n",
    "Y_pred = []\n",
    "for i in range(len(test_x_topics)):\n",
    "    tst = test_x_topics[i]\n",
    "    sim = [cossim(tst, tr_t) for tr_t in train_x_topics]\n",
    "    idx = np.array(sim).argsort()[-1]\n",
    "    Y_pred.append(Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8b8e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_lda = show_performance_data(Y_test, Y_pred, 'lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1018360",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vec = 100\n",
    "tagged_corpus = [TaggedDocument(d, [i]) for i, d in enumerate(X_train)]\n",
    "model = Doc2Vec(tagged_corpus, vector_size=vec, window=3, dm=1, min_count=1, workers=4)\n",
    "Y_pred = []\n",
    "for a in X_test:\n",
    "    test_doc_vector = model.infer_vector(a)\n",
    "    sims = model.docvecs.most_similar(positive = [test_doc_vector])\n",
    "    Y_pred.append(Y_train[sims[0][0]])\n",
    "    \n",
    "result_doc2vec = show_performance_data(Y_test, Y_pred,'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a491ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading USE encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "use_model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def get_use_vector(doc):\n",
    "    tmp = use_model(doc)\n",
    "    return np.mean(tmp, axis=0)\n",
    "# Convert text data into vector\n",
    "train_vectors = []\n",
    "for doc in X_train:\n",
    "    try:\n",
    "        train_vectors.append(get_use_vector(doc))\n",
    "    except Exception:\n",
    "        print('error...')\n",
    "test_vectors = []\n",
    "for doc in X_test:\n",
    "    try:\n",
    "        test_vectors.append(get_use_vector(doc))\n",
    "    except Exception:\n",
    "        print('error...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "cosine_similarities = linear_kernel(train_vectors, test_vectors)\n",
    "for cs in cosine_similarities.T:\n",
    "    idx = cs.argsort()[-1]\n",
    "    Y_pred.append(Y_train[idx])\n",
    "result_use = show_performance_data(Y_test, Y_pred, 'use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(train_vectors, Y_train)\n",
    "Y_pred = neigh.predict(test_vectors)\n",
    "result_knn = show_performance_data(Y_test, Y_pred, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(train_vectors, Y_train)\n",
    "Y_pred = clf.predict(test_vectors)\n",
    "result_rnmdfst = show_performance_data(Y_test, Y_pred, 'rndmfst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=15, learning_rate=1.0,max_depth=1, random_state=0).fit(train_vectors, Y_train)\n",
    "Y_pred = clf.predict(test_vectors)\n",
    "result_grdbst = show_performance_data(Y_test, Y_pred, 'grdbst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c279dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "results = {'Doc2Vec':result_doc2vec,  'KNN':result_knn, 'USE':result_use, 'Random Forest':result_rnmdfst, 'Gredient Boost':result_grdbst, 'LDA':result_lda}\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "p = []\n",
    "clr = ['b','g','r','c','m','k','y']\n",
    "i=0\n",
    "for a,b in results.items():\n",
    "    tmp = [b['Negative']['precision'], b['Neutral']['precision'], b['Positive']['precision']] # replace 'precision' with 'recall' or 'f1-score' for the next two plots.\n",
    "    print(tmp)\n",
    "    p.append(plt.bar(ind+width*i, tmp, width, color = clr[i]))\n",
    "    i+=1\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.ylabel('Precision')\n",
    "plt.title(\"Precison comparison of all the models with respect to sentiments\")\n",
    "plt.xticks(ind+width,labels)\n",
    "plt.legend( tuple(p), tuple(results.keys()), loc='upper center', ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 3\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "  \n",
    "r = []\n",
    "clr = ['b','g','r','c','m','k','y']\n",
    "i=0\n",
    "for a,b in results.items():\n",
    "    tmp = [b['Negative']['recall'], b['Neutral']['recall'], b['Positive']['recall']]\n",
    "    print(tmp)\n",
    "    r.append(plt.bar(ind+width*i, tmp, width, color = clr[i]))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.ylabel('Recall')\n",
    "plt.title(\"Recall comparison of all the models with respect to sentiments\")\n",
    "  \n",
    "plt.xticks(ind+width,labels)\n",
    "plt.legend( tuple(r), tuple(results.keys()), loc='upper center', ncol=2)\n",
    "#plt.show()\n",
    "plt.savefig('all_recall.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "  \n",
    "f = []\n",
    "clr = ['b','g','r','c','m','k','y']\n",
    "i=0\n",
    "for a,b in results.items():\n",
    "    tmp = [b['Negative']['f1-score'], b['Neutral']['f1-score'], b['Positive']['f1-score']]\n",
    "    print(tmp)\n",
    "    f.append(plt.bar(ind+width*i, tmp, width, color = clr[i]))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.ylabel('F1-score')\n",
    "plt.title(\"F score comparison of all the models with respect to sentiments\")\n",
    "  \n",
    "plt.xticks(ind+width,labels)\n",
    "plt.legend( tuple(f), tuple(results.keys()), loc='upper center', ncol=2)\n",
    "#plt.show()\n",
    "plt.savefig('all_f_score.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "f = []\n",
    "clr = ['b','g','r','c','m','k','y']\n",
    "i=0\n",
    "for a,b in results.items():\n",
    "    tmp = [b['weighted avg']['precision'], b['weighted avg']['recall'], b['weighted avg']['f1-score']]\n",
    "    print(tmp)\n",
    "    f.append(plt.bar(ind+width*i, tmp, width, color = clr[i]))\n",
    "    i+=1\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel('score')\n",
    "plt.title(\"Over all weighted scores\")\n",
    "  \n",
    "plt.xticks(ind+width,['Precision', 'Recall', 'F1-score'])\n",
    "plt.legend( tuple(f), tuple(results.keys()), loc='upper center', ncol=7)\n",
    "plt.show()\n",
    "plt.savefig('all_.png',bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

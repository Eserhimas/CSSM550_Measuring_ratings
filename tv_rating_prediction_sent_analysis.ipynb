{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the definition of the notebook\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755d0d5",
   "metadata": {},
   "source": [
    "## Importing Dataset and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Required libraries. Some of them may not be required.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "import pandas as pd #req.\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import xlsxwriter\n",
    "import re\n",
    "import string\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "import logging\n",
    "import string\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords, wordnet # req.\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from zemberek import ( #req.\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "import zeyrek\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Credentials to database connection\n",
    "hostname=\"***\"\n",
    "dbname=\"***\"\n",
    "uname=\"***\"\n",
    "pwd=\"***\"\n",
    "\n",
    "# Create SQLAlchemy engine to connect to MySQL Database\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\".format(host=hostname, db=dbname, user=uname, pw=pwd))\n",
    "\n",
    "q=\"SELECT * FROM alltwitter\"\n",
    "\n",
    "training=pd.read_sql_query(q, engine)\n",
    "\n",
    "df=df[['Tweet ID','Tweet','Time']]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data for visualization & exploration.\n",
    "data=pd.read_csv('tvseries_full.csv', delimiter=\";\", encoding='utf-8')\n",
    "#trainv = pd.read_csv('training_set.csv', delimiter=\";\", encoding='utf-8')\n",
    "#testv = pd.read_csv('test_set.csv', delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Lab.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data.Lab != 'Junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd483a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Lab.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb0e21",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f4b97",
   "metadata": {},
   "source": [
    "##### Removing Mentions, Punctuations, HTMLs, Hyperlinks and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].replace('@[A-Za-z0-9]+', '', regex=True).replace('@[A-Za-z0-9]+', '', regex=True)\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content']  = df['Content'] .replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content']  = df['Content'] .replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "df['Content']  = df['Content'] .replace('#[A-Za-z0-9ğüşöçıİĞÜŞÖÇ]+', \n",
    "                                  '', regex=True).replace('#[A-Za-z0-9ğüşöçıİĞÜŞÖÇ]+', '', regex=True)\n",
    "\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeated letters and spaces\n",
    "ss=df['Content'] .copy()\n",
    "new_st = []\n",
    "i = 0\n",
    "for k in ss:\n",
    "    new_ss = re.sub(r'([A-Za-z0-9ğüşöçıİĞÜŞÖÇ])\\1+', r'\\1',k)\n",
    "    new_st.append(new_ss)\n",
    "    i += 1\n",
    "\n",
    "df['Content'] = new_st\n",
    "\n",
    "df['Content']  = df['Content'].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f10c1d",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "extractor = TurkishSentenceExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef22e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "ss2 = df['Content'] .copy()\n",
    "new_st2 = []\n",
    "for k in ss2:\n",
    "    words = k.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    new_st2.append(stripped)\n",
    "    \n",
    "df['tokenized'] = new_st2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512679c2",
   "metadata": {},
   "source": [
    "##### Converting to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = df['tokenized'].apply(\n",
    "    lambda x: [word.lower() for word in x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "\n",
    "stops = set(stopwords.words('turkish'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords_removed'] = df['lower'].apply(\n",
    "    lambda x: [word for word in x if word not in stops])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c783ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying part of speech tags.\n",
    "\n",
    "df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67979b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting part of speeches to wordnet format.\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "df['wordnet_pos'] = df['pos_tags'].apply(\n",
    "    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wordnet_pos'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalization\n",
    "\n",
    "def normalize_long_text(text):\n",
    "    normalized_sentences = [normalizer.normalize(word) for word in text]\n",
    "    normalized_text = \" \".join(normalized_sentences)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['tokenized'].copy()\n",
    "new_sent = []\n",
    "start = time.time()\n",
    "\n",
    "for token in sentences:   \n",
    "    if token.count('') > 0:\n",
    "        token = list(filter(('').__ne__, token))\n",
    "    new_token = normalize_long_text(token)\n",
    "    new_sent.append(new_token)\n",
    "\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb479db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitted_words = []\n",
    "for sent in new_sent:\n",
    "    words = sent.split()\n",
    "    splitted_words.append(words)\n",
    "    \n",
    "for token in splitted_words:\n",
    "    j = 0\n",
    "    for word in token:\n",
    "        new_word = word.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", '')\n",
    "        token[j] = new_word\n",
    "        j += 1\n",
    "        \n",
    "        \n",
    "# Zeyrek for lemmatization\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "lem_sent = []\n",
    "for sent in splitted_words:\n",
    "    normalized_sent = []\n",
    "    for word in sent:\n",
    "        if word == '':\n",
    "            continue\n",
    "        else:\n",
    "            lem_word = analyzer.lemmatize(word)\n",
    "            normalized_sent.append(lem_word[0][1][0])\n",
    "    lem_sent.append(normalized_sent)\n",
    "    \n",
    "x = lem_sent.copy()\n",
    "for sent in x:\n",
    "    i = 0\n",
    "    for token in sent:\n",
    "        sent[i] = token.lower()\n",
    "        i += 1\n",
    "lem_sent = x\n",
    "\n",
    "\n",
    "lem_sent = list(filter(('').__ne__, lem_sent))\n",
    "\n",
    "df['lemmatized'] = lem_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_str'] = [' '.join(map(str, l)) for l in df['lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f74e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b67910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lab'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4567fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying target distribution.\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\n",
    "sns.countplot(df['Lab'], ax=axes[0])\n",
    "axes[1].pie(df['Lab'].value_counts(),\n",
    "            labels=['Positive', 'Neutral','Negative','Neutral-Negative'],\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=True,\n",
    "            explode=(0.05, 0, 0,0),\n",
    "            startangle=60)\n",
    "fig.suptitle('Distribution of the Tweets', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Character Count'] = df['Content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "def plot_dist3(df, feature, title):\n",
    "    # Creating a customized chart. and giving in figsize and everything.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n",
    "    # Creating a grid of 3 cols and 3 rows.\n",
    "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "\n",
    "    # Customizing the histogram grid.\n",
    "    ax1 = fig.add_subplot(grid[0, :2])\n",
    "    # Set the title.\n",
    "    ax1.set_title('Histogram')\n",
    "    # plot the histogram.\n",
    "    sns.distplot(df.loc[:, feature],\n",
    "                 hist=True,\n",
    "                 kde=True,\n",
    "                 ax=ax1,\n",
    "                 color='#e74c3c')\n",
    "    ax1.set(ylabel='Frequency')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "\n",
    "    # Customizing the ecdf_plot.\n",
    "    ax2 = fig.add_subplot(grid[1, :2])\n",
    "    # Set the title.\n",
    "    ax2.set_title('Empirical CDF')\n",
    "    # Plotting the ecdf_Plot.\n",
    "    sns.distplot(df.loc[:, feature],\n",
    "                 ax=ax2,\n",
    "                 kde_kws={'cumulative': True},\n",
    "                 hist_kws={'cumulative': True},\n",
    "                 color='#e74c3c')\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "    ax2.set(ylabel='Cumulative Probability')\n",
    "\n",
    "    # Customizing the Box Plot.\n",
    "    ax3 = fig.add_subplot(grid[:, 2])\n",
    "    # Set title.\n",
    "    ax3.set_title('Box Plot')\n",
    "    # Plotting the box plot.\n",
    "    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n",
    "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n",
    "\n",
    "    plt.suptitle(f'{title}', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_number_histogram(textneg, textpos, textneu,textneune):\n",
    "    \n",
    "    \"\"\"A function for comparing word counts\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=4, nrows=1, figsize=(18, 6), sharey=True)\n",
    "    sns.distplot(textneg.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n",
    "    sns.distplot(textpos.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n",
    "    sns.distplot(textneu.str.split().map(lambda x: len(x)), ax=axes[2], color='#e74c3c')\n",
    "    sns.distplot(textneu.str.split().map(lambda x: len(x)), ax=axes[3], color='#e74c3c')\n",
    "    \n",
    "    axes[0].set_xlabel('Word Count')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Negative Tweets')\n",
    "    axes[1].set_xlabel('Word Count')\n",
    "    axes[1].set_title('Positive Tweets')\n",
    "    axes[2].set_xlabel('Word Count')\n",
    "    axes[2].set_title('Neutral Tweets')\n",
    "    axes[3].set_xlabel('Word Count')\n",
    "    axes[3].set_title('Neutral-Negative Tweets')\n",
    "    \n",
    "    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_number_histogram(df[df['Lab'] == 'Negative']['Content'],\n",
    "                           df[df['Lab'] == 'Positive']['Content'],\n",
    "                          df[df['Lab'] == 'Neutral']['Content'],\n",
    "                          df[df['Lab'] == 'Neutral-Negative']['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1011016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.Lab)\n",
    "df['categorical_label'] = le.transform(df.Lab)\n",
    "\n",
    "display(df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['lemma_str']\n",
    "\n",
    "y=df['categorical_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.2, random_state=24,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBlobSentiment(Base):\n",
    "    \"\"\"Predict fine-grained sentiment classes using TextBlob.\"\"\"\n",
    "    def __init__(self, model_file: str=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def score(self, text: str) -> float:\n",
    "        # pip install textblob\n",
    "        from textblob import TextBlob\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    def predict(self, train_file: None, test_file: str, lower_case: bool) -> pd.DataFrame:\n",
    "        df = self.read_data(test_file, lower_case)\n",
    "        df['score'] = df['text'].apply(self.score)\n",
    "        # Convert float score to category based on binning\n",
    "        df['pred'] = pd.cut(df['score'],\n",
    "                            bins=5,\n",
    "                            labels=[1, 2, 3, 4, 5])\n",
    "        df = df.drop('score', axis=1)\n",
    "        return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
